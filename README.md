# LITA_Class_Documentation
This is where I documented my first project while learning data analysis on Incubator Hub

# EXCEL

## EXCEL FUNCTIONS 

I focused on Excel and worked with a dataset to calculate some key stats. I looked at the grand total and average salaries, pinpointed the highest and lowest salaries, and even found the fourth highest and third lowest salaries. I also played around with conditional functions, which allowed me to dig deeper into the data. I calculated the total salary for Bayelsa, the average salary for Oyo, the highest salary in Edo, the lowest in Taraba, and counted the staff in Nassarawa. Excel’s conditional functions really impressed me with how effectively they can target specific criteria!

![image](https://github.com/user-attachments/assets/87f87278-8ad0-433e-951e-93bbbf01b7d6)

![image](https://github.com/user-attachments/assets/f057e48c-1266-43c2-bc09-af2c9ad01618)


## WORKING WITH VLOOKUP

I explored Excel's VLOOKUP function while working on a salary structure analysis dataset. I used it to retrieve various salary components, such as basic, housing, transport, and leave allowances, all based on the employees' grade levels. This function helped ensure a quick and accurate breakdown of salaries across employees.

My focus was on using VLOOKUP to easily connect employee data with the salary structure, even when handling a large dataset. This allowed for more efficient gross pay calculations while maintaining data consistency across various departments and roles.

![image](https://github.com/user-attachments/assets/8e9d9824-3696-47b1-8ae9-da70f7e7e404)

![image](https://github.com/user-attachments/assets/e31c8fc5-df63-44b0-8f1f-aa69984625df)

![image](https://github.com/user-attachments/assets/1dcf6a3a-0b66-45cc-98be-f42d04e63281)


## Text Extraction Using Excel

I worked with some raw data using text extraction techniques in Excel. By utilizing functions like LEFT(), MID(), and RIGHT(), I was able to break down codes into department codes, purchase dates, and asset category codes. Text extraction is key in transforming raw data into meaningful, structured information for efficient analysis.

![image](https://github.com/user-attachments/assets/ead819cd-2964-440e-b87e-c837391cf11a)

![image](https://github.com/user-attachments/assets/a768c62a-e783-4c48-8518-6d463b4015b6)


## Data cleaning with Excel

I focused on data cleaning, and it truly makes a big difference when working with properly cleaned data. I combined first and last names to create full names, then used the first names to generate emails using Excel’s “ & “ function. These small steps make data more readable and manageable.

Data cleaning is one of the most important steps in data analysis. Just like an engineer gathering tools and clearing the workspace, data cleaning lays the foundation for reliable insights.

![image](https://github.com/user-attachments/assets/bd7edd13-2881-48a8-a69c-1b3e7b2a86ef)
![image](https://github.com/user-attachments/assets/96b66b81-8692-4efc-b52e-84e04df05247)
![image](https://github.com/user-attachments/assets/096ce4eb-4254-4d7f-a0f5-99958bd676f4)
![image](https://github.com/user-attachments/assets/59af3c3d-f6f7-4baf-93b1-2aa92bc4fa6e)


## Mastering Text Cleaning with Excel

The focus was on text cleaning and formatting in Excel. I worked through issues like inconsistent formatting, extra spaces, and pulling key details from strings. Using a mix of Excel functions, I turned messy raw data into something much cleaner and easier to work with, without losing any important details.
I used PROPER, UPPER, and LOWER functions to make sure names had consistent capitalization. The TRIM function came in handy to get rid of those annoying extra spaces, making everything more readable. I also used LEFT to pull specific info like first names or surnames, which made the data easier to manage.
Having clean data makes everything so much smoother and even more accurate.

![image](https://github.com/user-attachments/assets/31b6c8a0-8f67-4a2d-8fdb-dcde5bf33f39)
![image](https://github.com/user-attachments/assets/e4fafc08-d67d-4bec-82bb-e5135af39b0d)
![image](https://github.com/user-attachments/assets/a01362c3-ab5f-4b7f-b21d-1bcaf6e32c44)
![image](https://github.com/user-attachments/assets/d498149a-1fc0-4c59-82d7-519dfa1d7999)


## Unlocking Insights with Pivot Tables

I spent time using pivot tables in Excel to explore sales across different regions and stores. Pivot tables make it easy to arrange and summarize large amounts of data, so I could quickly see which areas were doing well:

1. Top Region: North East earned ₦18.6 billion.

2. Best Store: Ankpa had the highest sales with ₦1.7 billion.

This tool helps me break down data and find patterns that might be missed. It’s a simple way to understand what’s working and what needs attention. 

![image](https://github.com/user-attachments/assets/fb285a50-39c6-477d-bb49-6c7adb708af2)
![image](https://github.com/user-attachments/assets/374aaa9d-4650-4f02-8434-673c37d39be0)

## Unlocking Insights with Pivot Tables

I worked with pivot tables in Excel to analyze the sales and revenue data, and the insights were eye-opening!

Top Markets by Revenue: Ekiti came out on top with ₦5.57 billion, with Abia and Bayelsa not far behind. Together, the top 10 markets generated over ₦22 billion—pretty impressive!

Bottom 5 Stores by Units Sold: Some stores like Boki and Kwali had lower sales, which could mean room for growth or areas to re-evaluate.

Regional Revenue Averages: It’s interesting to see that the South South region has the highest average revenue, while North Central comes in lower at around ₦3.5 million. These patterns might reveal something about customer demand in each region.

Line of Business Breakdown: Service Plans really lead in revenue, with Copier and Printer Sales close behind. Knowing which areas drive revenue can help pinpoint where to focus future efforts.



![image](https://github.com/user-attachments/assets/875b3415-1ead-4955-8abb-33cf3ea8f272)
![image](https://github.com/user-attachments/assets/0cc4692d-2f81-4ed6-bba8-2fc12556f373)


# SQL

## Building Foundations with SQL Tables and Queries

Today, I worked on creating and managing tables in SQL Server. I created an Employee table with details like names, gender, birth dates, and hire dates, and practiced inserting records. Each query I run teaches me something new!

The "staffid" serves as the primary key, ensuring each employee record is unique—a fundamental yet crucial aspect of database design! Adding records for different employees was a good exercise in understanding data structures and maintaining consistency.

I used SELECT statements to retrieve specific columns like staff IDs and first names. These queries show how to access and organize data with precision. This hands-on experience with tables and primary keys is a reminder of how SQL is essential for real-world data management and analysis.

![image](https://github.com/user-attachments/assets/df7a4165-5164-4052-bef1-ba4eacca8816)
![image](https://github.com/user-attachments/assets/16bf9dea-9ae3-48bb-ae44-1e8c5bfb9794)
![image](https://github.com/user-attachments/assets/c203a2c4-070e-426b-8e20-89281533a70c)











